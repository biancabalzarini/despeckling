{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jugando con autoenconders\n",
    "Esta notebook tiene el único objetivo de hacer primeras pruebas con autoencoders. Se puede correr en Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura del autoencoder\n",
    "class Autoencoder(nn.Module): # La clase Autoencoder hereda de la clase nn.Module, que es una clase base para todos los modelos en PyTorch.\n",
    "                              # Esto permite que nuestra clase Autoencoder tenga todas las funcionalidades necesarias para ser un modelo de aprendizaje profundo en PyTorch.\n",
    "\n",
    "    # Dentro del método __init__, definimos las capas del autoencoder.\n",
    "    def __init__(self, encoding_dim):\n",
    "\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential( # Encoder\n",
    "                                      # Toma una imagen de entrada y la comprime en una representación de dimensionalidad más baja llamada encoding_dim\n",
    "\n",
    "            # Secuencia de capas del codificador:\n",
    "            nn.Linear(28 * 28, 128), # Capa lineal inicial que toma una imagen de 28x28 píxeles (784 dimensiones después de aplanarla) y la reduce a 128 dimensiones utilizando una función lineal.\n",
    "            nn.ReLU(), # Luego se aplica una función de activación ReLU para introducir no linealidad en la representación.\n",
    "            nn.Linear(128, encoding_dim), # Finalmente, otra capa lineal reduce la dimensionalidad a encoding_dim.\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential( # Decoder\n",
    "                                      # Devuelve la imágen a su tamaño original.\n",
    "\n",
    "            # Secuencia de capas del decodificador:\n",
    "            nn.Linear(encoding_dim, 128), # Capa lineal que toma la representación de encoding_dim y la expande a 128 dimensiones.\n",
    "            nn.ReLU(), # Luego, se aplica una función de activación ReLU.\n",
    "            nn.Linear(128, 28 * 28), # A continuación, otra capa lineal expande la dimensionalidad a 28x28 píxeles (784 dimensiones).\n",
    "            nn.Sigmoid(), # Finalmente se aplica una función de activación sigmoide para limitar los valores de salida entre 0 y 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # El método forward define cómo se propagan los datos a través del autoencoder.\n",
    "\n",
    "        encoded = self.encoder(x) # Toma una imagen de entrada x, la pasa por el codificador para obtener la representación comprimida encoded.\n",
    "        decoded = self.decoder(encoded) # Luego pasa esta representación por el decodificador para obtener la reconstrucción decoded.\n",
    "        return decoded # La reconstrucción se devuelve como salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de hiperparámetros\n",
    "encoding_dim = 32\n",
    "batch_size = 128 # Cantidad de muestras que se procesan en paralelo antes de que se actualicen los pesos del modelo.\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.6%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "27.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de entrenamiento (MNIST)\n",
    "transform = transforms.Compose([transforms.ToTensor()]) # Convertir las imágenes en tensores de tipo torch.Tensor, lo que permite el procesamiento eficiente de los datos en el modelo.\n",
    "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True) # Crea una instancia del conjunto de datos MNIST para el entrenamiento.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Aquí se crea un objeto DataLoader, que es responsable de cargar los datos en el modelo durante el entrenamiento.\n",
    "                                                                              # train_dataset es el conjunto de datos que se va a cargar.\n",
    "                                                                              # batch_size indica el número de ejemplos que se cargarán juntos en cada iteración.\n",
    "                                                                              # shuffle=True significa que los datos se barajarán aleatoriamente en cada época de entrenamiento para introducir variedad y evitar sesgos en el orden de los ejemplos.\n",
    "\n",
    "# Solo como comentario, si quiero hacerle varias transformaciones a las imagenes, puedo usar el compose para hacerlas todas seguidas.\n",
    "# Aca hay unos ejemplos de las cosas que se pueden hacer con transforms:\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.Resize((64, 64)),   # Cambiar tamaño a 64x64\n",
    "#    transforms.ToTensor(),         # Convertir a tensor\n",
    "#    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalización\n",
    "#])\n",
    "\n",
    "\n",
    "# Info util sobre DataLoders y DataSets, para cargar eficientemente el dataset de imagenes\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizamos una de las imágenes del set de entrenamiento solo a modo de ejemplo.\n",
    "image, label = train_dataset[348]\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la instancia del autoencoder\n",
    "autoencoder = Autoencoder(encoding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.BCELoss()  # Utilizamos Binary Cross Entropy Loss como loss function ya que las imágenes están normalizadas en el rango [0, 1]\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate) # El optimizador es responsable de ajustar los pesos del modelo con el fin de minimizar la función de pérdida.\n",
    "                                                                   # Adam es un algoritmo de optimización popular y eficiente que adapta la tasa de aprendizaje de forma dinámica para cada parámetro del modelo.\n",
    "                                                                   # La tasa de aprendizaje determina qué tan rápido se ajustan los pesos del modelo durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1398\n",
      "Epoch [2/10], Loss: 0.1120\n",
      "Epoch [3/10], Loss: 0.0998\n",
      "Epoch [4/10], Loss: 0.1003\n",
      "Epoch [5/10], Loss: 0.0930\n",
      "Epoch [6/10], Loss: 0.0886\n",
      "Epoch [7/10], Loss: 0.0917\n",
      "Epoch [8/10], Loss: 0.0824\n",
      "Epoch [9/10], Loss: 0.0866\n",
      "Epoch [10/10], Loss: 0.0897\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del autoencoder\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        images, _ = data\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = autoencoder(images) # Se pasa a las imágenes por el autoencoder, en una pasada forward.\n",
    "        loss = criterion(outputs, images) # Se calcula la diferencia entre el output y las imágenes originales, según la función de pérdida definida.\n",
    "\n",
    "        # Backward pass y optimización\n",
    "        optimizer.zero_grad() # Se restablecen los gradientes acumulados en todos los parámetros del modelo.\n",
    "                              # Esto es necesario antes de realizar el backward pass, ya que PyTorch acumula los gradientes en cada llamada a loss.backward().\n",
    "        loss.backward() # Se realiza el backward pass para calcular los gradientes de los parámetros del autoencoder utilizando la función de pérdida.\n",
    "        optimizer.step() # Finalmente se realiza la optimización de los parámetros del modelo mediante la llamada a optimizer.step(), que actualiza los parámetros en función de los gradientes calculados.\n",
    "\n",
    "    # Imprimir la pérdida del autoencoder en cada época\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.0847\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del autoencoder\n",
    "test_dataset = MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "total_loss = 0\n",
    "with torch.no_grad(): # Esto es para asegurarse de que no se realicen cálculos de gradientes durante la evaluación del autoencoder.\n",
    "                      # Al entrar en este bloque, se desactiva el cálculo y almacenamiento automático de gradientes para reducir el uso de memoria y acelerar la evaluación.\n",
    "    for data in test_loader:\n",
    "        images, _ = data # _ se utiliza para descartar las etiquetas, ya que no son necesarias para la evaluación.\n",
    "        images = images.view(images.size(0), -1) # Se modifica la forma de las imágenes para que coincida con el formato esperado por el autoencoder.\n",
    "                                                 # En este caso, las imágenes se aplanan en un tensor unidimensional. images.size(0) se utiliza para obtener el tamaño del lote.\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = autoencoder(images) # Se realiza el forward pass del autoencoder con las imágenes de prueba.\n",
    "                                      # El autoencoder genera las imágenes reconstruidas utilizando el método forward() que definimos previamente en la clase Autoencoder.\n",
    "        loss = criterion(outputs, images) # Se calcula la pérdida entre las imágenes reconstruidas y las imágenes originales utilizando la función de pérdida (criterion).\n",
    "                                          # Esto proporciona una medida de cuánto difieren las imágenes reconstruidas de las originales.\n",
    "        total_loss += loss.item() # La pérdida obtenida en cada iteración se suma a la variable total_loss utilizando loss.item(), que devuelve el valor escalar de la pérdida.\n",
    "                                  # Al final de la iteración, total_loss contendrá la suma acumulada de las pérdidas de todas las muestras del conjunto de datos de prueba.\n",
    "\n",
    "average_loss = total_loss / len(test_loader) # Se calcula la pérdida promedio dividiendo la suma acumulada de las pérdidas (total_loss) entre el número de lotes en el conjunto de datos de prueba (len(test_loader)).\n",
    "                                             # Esto proporciona una medida promedio de la discrepancia entre las imágenes originales y las imágenes reconstruidas por el autoencoder en el conjunto de datos de prueba.\n",
    "print(f\"Average Test Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ahora aplicamos el autoencoder a un ejemplo particular del dataset de testeo y vemos cómo queda la imagen reconstriuda.\n",
    "\n",
    "index = 978  # Índice del ejemplo puntual que se desea seleccionar\n",
    "example = test_dataset[index][0]  # Obtiene la imagen original del ejemplo\n",
    "\n",
    "example = example.view(1, -1)  # Ajusta la forma de la imagen a un lote de tamaño 1\n",
    "\n",
    "reconstructed = autoencoder(example)  # Aplica el autoencoder al ejemplo\n",
    "\n",
    "original = example.view(28, 28)  # Ajusta la forma de la imagen original\n",
    "reconstructed = reconstructed.view(28, 28)  # Ajusta la forma de la imagen reconstruida\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original, cmap='gray')\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed.detach(), cmap='gray')\n",
    "plt.title('Reconstructed')\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejo este pedazo de código para tener una idea de cómo habría que cambiar el entrenamiento para que compare la salida del autoencoder con las imágenes limpias en vez de con las imágenes ruidosas del input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Dentro del bucle de entrenamiento\n",
    "for data in train_loader:\n",
    "    noisy_images, clean_images = data\n",
    "    noisy_images = noisy_images.view(noisy_images.size(0), -1)\n",
    "    clean_images = clean_images.view(clean_images.size(0), -1)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = autoencoder(noisy_images)\n",
    "    loss = criterion(outputs, clean_images)\n",
    "\n",
    "    # Backward pass y optimización\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "despeckling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
