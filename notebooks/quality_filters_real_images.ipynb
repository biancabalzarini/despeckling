{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb84601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from scripts.autoencoders import InMemoryImageDataset, split_images, ConfigurableAutoencoder\n",
    "from scripts.measuring_quality import first_order_method, co_ocurrence_matrix, deltah, second_order_method\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from omegaconf import OmegaConf\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b71f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    OmegaConf.register_new_resolver(\"eval\", eval)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7227f",
   "metadata": {},
   "source": [
    "Elegir el archivo de configuración correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547db7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': {'num_epochs': 300, 'batch_size': 64, 'learning_rate': 0.001, 'scheduler_name': 'elr', 'scheduler_params': {'gamma': 0.95}, 'side_size': 512}, 'model': {'encoding_dim': 1600, 'loss_function': 'mse', 'optimizer': 'adam'}, 'encoder': {'layers': [{'type': 'conv2d', 'filters': 8, 'kernel_size': 8, 'stride': 4, 'padding': 2, 'activation': 'relu'}, {'type': 'conv2d', 'filters': 16, 'kernel_size': 8, 'stride': 4, 'padding': 2, 'activation': 'relu'}, {'type': 'flatten'}, {'type': 'dense', 'dim': '${model.encoding_dim}', 'activation': 'relu'}]}, 'decoder': {'layers': [{'type': 'dense', 'dim': 16384, 'activation': 'relu'}, {'type': 'unflatten', 'dim1': 32, 'dim2': 32, 'out_channels': 16}, {'type': 'conv2d_transpose', 'filters': 8, 'kernel_size': 8, 'stride': 4, 'padding': 2, 'activation': 'relu'}, {'type': 'conv2d_transpose', 'filters': 1, 'kernel_size': 8, 'stride': 4, 'padding': 2, 'activation': 'sigmoid'}]}, 'testing': {'batch_size': 32}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_name = 'config_1' # Elegir\n",
    "\n",
    "config_path = f'configs/imagenes_reales/{config_name}.yaml'\n",
    "config = OmegaConf.load(config_path)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107a217",
   "metadata": {},
   "source": [
    "Cargo el autoencoder ya entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128f64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = config['training']['side_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d68c5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfigurableAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(8, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    (5): Linear(in_features=16384, out_features=1600, bias=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=1600, out_features=16384, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Unflatten(dim=1, unflattened_size=(16, 32, 32))\n",
       "    (3): ConvTranspose2d(16, 8, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(8, 1, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Crear una instancia del modelo (debe tener la misma arquitectura)\n",
    "autoencoder_cargado = ConfigurableAutoencoder(config=config, image_size=image_size)\n",
    "# 2. Carga los parámetros\n",
    "autoencoder_cargado.load_state_dict(torch.load(f'data/trained_models/imagenes_reales/{config_name}.pth'))\n",
    "# 3. Modo evaluación (cuando lo use para inferencia)\n",
    "autoencoder_cargado.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b48ca",
   "metadata": {},
   "source": [
    "Genero dataset de testeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bd2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiff_datasets(carpeta):\n",
    "    carpeta_archivos = f\"C:/Users/Bianca/Documents/Archivos/ITBA/Tesis_Maestria/SAR_despeckling_filters_dataset/Main_folder/{carpeta}/\"\n",
    "    archivos_tiff = [f for f in os.listdir(carpeta_archivos) if f.endswith('.tiff')]\n",
    "\n",
    "    num_imagenes = len(archivos_tiff)\n",
    "    array_final = np.empty((num_imagenes, 512, 512), dtype=np.uint8)\n",
    "\n",
    "    for i, archivo in enumerate(archivos_tiff):\n",
    "        imagen = Image.open(os.path.join(carpeta_archivos, archivo))\n",
    "        array_imagen = np.array(imagen)[:, :, 0]\n",
    "        array_final[i] = array_imagen\n",
    "        \n",
    "        if (i+1) % 250 == 0:\n",
    "            print(f'{i+1} de {num_imagenes} imágenes procesadas')\n",
    "\n",
    "    print(f'\\nArray final shape: {array_final.shape}')\n",
    "    print(f'Máximo valor: {array_final.max()}, mínimo valor: {array_final.min()}')\n",
    "    \n",
    "    return array_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56860e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Array final shape: (100, 512, 512)\n",
      "Máximo valor: 255, mínimo valor: 0\n"
     ]
    }
   ],
   "source": [
    "noisy_val = load_tiff_datasets('Noisy_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16589f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Array final shape: (100, 512, 512)\n",
      "Máximo valor: 255, mínimo valor: 0\n"
     ]
    }
   ],
   "source": [
    "clean_val = load_tiff_datasets('GTruth_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a844aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 512, 512)\n",
      "(100, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "clean_val = clean_val[np.array([np.min(img) != np.max(img) for img in clean_val])]\n",
    "noisy_val = noisy_val[np.array([np.min(img) != np.max(img) for img in noisy_val])]\n",
    "print(clean_val.shape)\n",
    "print(noisy_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb3c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_cargado.eval() # Para desactivar Dropout, BatchNorm, etc.\n",
    "batch_size = config['testing']['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d90ba978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 512, 512)\n",
      "(100, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "if config['training']['side_size'] == 512:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    clean_val = split_images(clean_val, config['training']['side_size'])\n",
    "    noisy_val = split_images(noisy_val, config['training']['side_size'])    \n",
    "    \n",
    "    clean_val = clean_val[np.array([np.min(img) != np.max(img) for img in clean_val])]\n",
    "    noisy_val = noisy_val[np.array([np.min(img) != np.max(img) for img in noisy_val])]\n",
    "    \n",
    "print(clean_val.shape)\n",
    "print(noisy_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e9e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_to_01 = transforms.Lambda(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_to_01\n",
    "])\n",
    "\n",
    "dataset_test = InMemoryImageDataset(noisy_val, clean_val, transform=transform)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "despeckling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
